{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "13f71cb7",
      "metadata": {
        "id": "13f71cb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/jeffrey-22/ads.git\n",
            "  Cloning https://github.com/jeffrey-22/ads.git to c:\\users\\86189\\appdata\\local\\temp\\pip-req-build-tzie631s\n",
            "  Resolved https://github.com/jeffrey-22/ads.git to commit dd5df9c15ba4fceb66be5891351c027be5bfd142\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting pandas (from fynesse==0.1.1)\n",
            "  Using cached pandas-2.1.3-cp312-cp312-win_amd64.whl.metadata (18 kB)\n",
            "Collecting numpy (from fynesse==0.1.1)\n",
            "  Using cached numpy-1.26.2-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
            "Collecting jupyter (from fynesse==0.1.1)\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting matplotlib (from fynesse==0.1.1)\n",
            "  Using cached matplotlib-3.8.2-cp312-cp312-win_amd64.whl.metadata (5.9 kB)\n",
            "Collecting pyyaml (from fynesse==0.1.1)\n",
            "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting pymysql (from fynesse==0.1.1)\n",
            "  Using cached PyMySQL-1.1.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting wget (from fynesse==0.1.1)\n",
            "  Using cached wget-3.2-py3-none-any.whl\n",
            "Collecting dask (from fynesse==0.1.1)\n",
            "  Using cached dask-2023.11.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting click>=8.1 (from dask->fynesse==0.1.1)\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting cloudpickle>=1.5.0 (from dask->fynesse==0.1.1)\n",
            "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting fsspec>=2021.09.0 (from dask->fynesse==0.1.1)\n",
            "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting packaging>=20.0 (from dask->fynesse==0.1.1)\n",
            "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting partd>=1.2.0 (from dask->fynesse==0.1.1)\n",
            "  Using cached partd-1.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting toolz>=0.10.0 (from dask->fynesse==0.1.1)\n",
            "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
            "Collecting importlib-metadata>=4.13.0 (from dask->fynesse==0.1.1)\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting notebook (from jupyter->fynesse==0.1.1)\n",
            "  Using cached notebook-7.0.6-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting qtconsole (from jupyter->fynesse==0.1.1)\n",
            "  Using cached qtconsole-5.5.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting jupyter-console (from jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
            "Collecting nbconvert (from jupyter->fynesse==0.1.1)\n",
            "  Using cached nbconvert-7.11.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting ipykernel (from jupyter->fynesse==0.1.1)\n",
            "  Using cached ipykernel-6.26.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ipywidgets (from jupyter->fynesse==0.1.1)\n",
            "  Using cached ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->fynesse==0.1.1)\n",
            "  Using cached contourpy-1.2.0-cp312-cp312-win_amd64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->fynesse==0.1.1)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->fynesse==0.1.1)\n",
            "  Using cached fonttools-4.44.3-cp312-cp312-win_amd64.whl.metadata (157 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->fynesse==0.1.1)\n",
            "  Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
            "Collecting pillow>=8 (from matplotlib->fynesse==0.1.1)\n",
            "  Using cached Pillow-10.1.0-cp312-cp312-win_amd64.whl.metadata (9.6 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->fynesse==0.1.1)\n",
            "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->fynesse==0.1.1)\n",
            "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->fynesse==0.1.1)\n",
            "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas->fynesse==0.1.1)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Collecting colorama (from click>=8.1->dask->fynesse==0.1.1)\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting zipp>=0.5 (from importlib-metadata>=4.13.0->dask->fynesse==0.1.1)\n",
            "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting locket (from partd>=1.2.0->dask->fynesse==0.1.1)\n",
            "  Using cached locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->fynesse==0.1.1)\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached comm-0.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached debugpy-1.8.0-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting ipython>=7.23.1 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached ipython-8.17.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyter_client-8.6.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyter_core-5.5.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting matplotlib-inline>=0.1 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting nest-asyncio (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached nest_asyncio-1.5.8-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting psutil (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached psutil-5.9.6-cp37-abi3-win_amd64.whl.metadata (22 kB)\n",
            "Collecting pyzmq>=20 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached pyzmq-25.1.1-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
            "Collecting tornado>=6.1 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached tornado-6.3.3-cp38-abi3-win_amd64.whl.metadata (2.6 kB)\n",
            "Collecting traitlets>=5.4.0 (from ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached traitlets-5.13.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting widgetsnbextension~=4.0.9 (from ipywidgets->jupyter->fynesse==0.1.1)\n",
            "  Using cached widgetsnbextension-4.0.9-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab-widgets~=3.0.9 (from ipywidgets->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyterlab_widgets-3.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting prompt-toolkit>=3.0.30 (from jupyter-console->jupyter->fynesse==0.1.1)\n",
            "  Using cached prompt_toolkit-3.0.41-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pygments (from jupyter-console->jupyter->fynesse==0.1.1)\n",
            "  Downloading pygments-2.17.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting beautifulsoup4 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
            "Collecting bleach!=5.0.0 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting defusedxml (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting jinja2>=3.0 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting jupyterlab-pygments (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting markupsafe>=2.0 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached MarkupSafe-2.1.3-cp312-cp312-win_amd64.whl.metadata (3.0 kB)\n",
            "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting nbclient>=0.5.0 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached nbclient-0.9.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting nbformat>=5.7 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached nbformat-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting tinycss2 (from nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyter_server-2.10.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.22.1 (from notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyterlab_server-2.25.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jupyterlab<5,>=4.0.2 (from notebook->jupyter->fynesse==0.1.1)\n",
            "  Downloading jupyterlab-4.0.9-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting notebook-shim<0.3,>=0.2 (from notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter->fynesse==0.1.1)\n",
            "  Using cached QtPy-2.4.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting webencodings (from bleach!=5.0.0->nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting decorator (from ipython>=7.23.1->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting stack-data (from ipython>=7.23.1->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting platformdirs>=2.5 (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached platformdirs-4.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pywin32>=300 (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached pywin32-306-cp312-cp312-win_amd64.whl (9.2 MB)\n",
            "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached anyio-4.0.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting argon2-cffi (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyter_events-0.9.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jupyter-server-terminals (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
            "Collecting overrides (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached overrides-7.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting prometheus-client (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached prometheus_client-0.18.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting pywinpty (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached pywinpty-2.0.12-cp312-none-win_amd64.whl.metadata (5.2 kB)\n",
            "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
            "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached terminado-0.18.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting websocket-client (from jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached websocket_client-1.6.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab<5,>=4.0.2->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab<5,>=4.0.2->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached jupyter_lsp-2.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached Babel-2.13.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached json5-0.9.14-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached jsonschema-4.20.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting requests>=2.31 (from jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting fastjsonschema (from nbformat>=5.7->nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached fastjsonschema-2.19.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting wcwidth (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->fynesse==0.1.1)\n",
            "  Using cached wcwidth-0.2.10-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter->fynesse==0.1.1)\n",
            "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting idna>=2.8 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting setuptools (from babel>=2.10->jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached jsonschema_specifications-2023.11.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached referencing-0.31.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached rpds_py-0.13.0-cp312-none-win_amd64.whl.metadata (3.8 kB)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting argon2-cffi-bindings (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached executing-2.0.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel->jupyter->fynesse==0.1.1)\n",
            "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached webcolors-1.13-py3-none-any.whl (14 kB)\n",
            "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
            "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter->fynesse==0.1.1)\n",
            "  Using cached types_python_dateutil-2.8.19.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Using cached dask-2023.11.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
            "Using cached matplotlib-3.8.2-cp312-cp312-win_amd64.whl (7.6 MB)\n",
            "Using cached numpy-1.26.2-cp312-cp312-win_amd64.whl (15.5 MB)\n",
            "Using cached pandas-2.1.3-cp312-cp312-win_amd64.whl (10.5 MB)\n",
            "Using cached PyMySQL-1.1.0-py3-none-any.whl (44 kB)\n",
            "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Using cached contourpy-1.2.0-cp312-cp312-win_amd64.whl (187 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.44.3-cp312-cp312-win_amd64.whl (2.1 MB)\n",
            "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Using cached kiwisolver-1.4.5-cp312-cp312-win_amd64.whl (56 kB)\n",
            "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "Using cached partd-1.4.1-py3-none-any.whl (18 kB)\n",
            "Using cached Pillow-10.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
            "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "Using cached ipykernel-6.26.0-py3-none-any.whl (114 kB)\n",
            "Using cached ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
            "Using cached nbconvert-7.11.0-py3-none-any.whl (256 kB)\n",
            "Using cached notebook-7.0.6-py3-none-any.whl (4.0 MB)\n",
            "Using cached qtconsole-5.5.1-py3-none-any.whl (123 kB)\n",
            "Using cached bleach-6.1.0-py3-none-any.whl (162 kB)\n",
            "Using cached comm-0.2.0-py3-none-any.whl (7.0 kB)\n",
            "Using cached debugpy-1.8.0-py2.py3-none-any.whl (5.0 MB)\n",
            "Using cached ipython-8.17.2-py3-none-any.whl (808 kB)\n",
            "Using cached jupyter_client-8.6.0-py3-none-any.whl (105 kB)\n",
            "Using cached jupyter_core-5.5.0-py3-none-any.whl (28 kB)\n",
            "Using cached jupyter_server-2.10.1-py3-none-any.whl (378 kB)\n",
            "Downloading jupyterlab-4.0.9-py3-none-any.whl (9.2 MB)\n",
            "   ---------------------------------------- 0.0/9.2 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.7/9.2 MB 22.5 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 2.5/9.2 MB 32.3 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 4.1/9.2 MB 29.0 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 5.7/9.2 MB 30.6 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 7.3/9.2 MB 31.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.0/9.2 MB 32.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.2/9.2 MB 30.9 MB/s eta 0:00:00\n",
            "Using cached jupyterlab_server-2.25.2-py3-none-any.whl (58 kB)\n",
            "Using cached jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
            "Using cached MarkupSafe-2.1.3-cp312-cp312-win_amd64.whl (16 kB)\n",
            "Using cached mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "Using cached nbclient-0.9.0-py3-none-any.whl (24 kB)\n",
            "Using cached nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
            "Using cached prompt_toolkit-3.0.41-py3-none-any.whl (385 kB)\n",
            "Downloading pygments-2.17.1-py3-none-any.whl (1.2 MB)\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.2/1.2 MB 24.9 MB/s eta 0:00:00\n",
            "Using cached pyzmq-25.1.1-cp312-cp312-win_amd64.whl (1.3 MB)\n",
            "Using cached QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
            "Using cached tornado-6.3.3-cp38-abi3-win_amd64.whl (429 kB)\n",
            "Using cached traitlets-5.13.0-py3-none-any.whl (84 kB)\n",
            "Using cached widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
            "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
            "Using cached nest_asyncio-1.5.8-py3-none-any.whl (5.3 kB)\n",
            "Using cached psutil-5.9.6-cp37-abi3-win_amd64.whl (252 kB)\n",
            "Using cached anyio-4.0.0-py3-none-any.whl (83 kB)\n",
            "Using cached async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Using cached Babel-2.13.1-py3-none-any.whl (10.1 MB)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Using cached json5-0.9.14-py2.py3-none-any.whl (19 kB)\n",
            "Using cached jsonschema-4.20.0-py3-none-any.whl (84 kB)\n",
            "Using cached jupyter_events-0.9.0-py3-none-any.whl (18 kB)\n",
            "Using cached jupyter_lsp-2.2.0-py3-none-any.whl (65 kB)\n",
            "Using cached platformdirs-4.0.0-py3-none-any.whl (17 kB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Using cached terminado-0.18.0-py3-none-any.whl (14 kB)\n",
            "Using cached pywinpty-2.0.12-cp312-none-win_amd64.whl (1.4 MB)\n",
            "Using cached argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
            "Using cached fastjsonschema-2.19.0-py3-none-any.whl (23 kB)\n",
            "Using cached overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Using cached prometheus_client-0.18.0-py3-none-any.whl (61 kB)\n",
            "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Using cached wcwidth-0.2.10-py2.py3-none-any.whl (105 kB)\n",
            "Using cached websocket_client-1.6.4-py3-none-any.whl (57 kB)\n",
            "Using cached asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
            "Using cached executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Using cached jsonschema_specifications-2023.11.1-py3-none-any.whl (17 kB)\n",
            "Using cached referencing-0.31.0-py3-none-any.whl (25 kB)\n",
            "Using cached rpds_py-0.13.0-cp312-none-win_amd64.whl (189 kB)\n",
            "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "Using cached setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
            "Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl (181 kB)\n",
            "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "Using cached types_python_dateutil-2.8.19.14-py3-none-any.whl (9.4 kB)\n",
            "Building wheels for collected packages: fynesse\n",
            "  Building wheel for fynesse (setup.py): started\n",
            "  Building wheel for fynesse (setup.py): finished with status 'done'\n",
            "  Created wheel for fynesse: filename=fynesse-0.1.1-py3-none-any.whl size=8299 sha256=416f89bb0196154ba5b804396cc206b79128d83440ccab10f1d7269a7fcb74b6\n",
            "  Stored in directory: C:\\Users\\86189\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-r2sw9lvf\\wheels\\38\\77\\18\\c5a4cb49b4a82b5f8ed392ba72ede299e17042437cb6cf7f59\n",
            "Successfully built fynesse\n",
            "Installing collected packages: wget, webencodings, wcwidth, types-python-dateutil, pywin32, pytz, pure-eval, json5, fastjsonschema, zipp, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, tzdata, traitlets, tornado, toolz, tinycss2, soupsieve, sniffio, six, setuptools, send2trash, rpds-py, rfc3986-validator, pyzmq, pyyaml, pywinpty, python-json-logger, pyparsing, pymysql, pygments, pycparser, psutil, prompt-toolkit, prometheus-client, platformdirs, pillow, parso, pandocfilters, packaging, overrides, numpy, nest-asyncio, mistune, markupsafe, locket, kiwisolver, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, idna, fsspec, fqdn, fonttools, executing, defusedxml, decorator, debugpy, cycler, colorama, cloudpickle, charset-normalizer, certifi, attrs, async-lru, terminado, rfc3339-validator, requests, referencing, qtpy, python-dateutil, partd, matplotlib-inline, jupyter-core, jinja2, jedi, importlib-metadata, contourpy, comm, click, cffi, bleach, beautifulsoup4, babel, asttokens, anyio, stack-data, pandas, matplotlib, jupyter-server-terminals, jupyter-client, jsonschema-specifications, dask, arrow, argon2-cffi-bindings, jsonschema, isoduration, ipython, argon2-cffi, nbformat, ipywidgets, ipykernel, qtconsole, nbclient, jupyter-events, jupyter-console, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter, fynesse\n",
            "  Attempting uninstall: wget\n",
            "    Found existing installation: wget 3.2\n",
            "    Uninstalling wget-3.2:\n",
            "      Successfully uninstalled wget-3.2\n",
            "  Attempting uninstall: webencodings\n",
            "    Found existing installation: webencodings 0.5.1\n",
            "    Uninstalling webencodings-0.5.1:\n",
            "      Successfully uninstalled webencodings-0.5.1\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.10\n",
            "    Uninstalling wcwidth-0.2.10:\n",
            "      Successfully uninstalled wcwidth-0.2.10\n",
            "  Attempting uninstall: types-python-dateutil\n",
            "    Found existing installation: types-python-dateutil 2.8.19.14\n",
            "    Uninstalling types-python-dateutil-2.8.19.14:\n",
            "      Successfully uninstalled types-python-dateutil-2.8.19.14\n",
            "  Attempting uninstall: pywin32\n",
            "    Found existing installation: pywin32 306\n",
            "    Uninstalling pywin32-306:\n",
            "      Successfully uninstalled pywin32-306\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2023.3.post1\n",
            "    Uninstalling pytz-2023.3.post1:\n",
            "      Successfully uninstalled pytz-2023.3.post1\n",
            "  Attempting uninstall: pure-eval\n",
            "    Found existing installation: pure-eval 0.2.2\n",
            "    Uninstalling pure-eval-0.2.2:\n",
            "      Successfully uninstalled pure-eval-0.2.2\n",
            "  Attempting uninstall: json5\n",
            "    Found existing installation: json5 0.9.14\n",
            "    Uninstalling json5-0.9.14:\n",
            "      Successfully uninstalled json5-0.9.14\n",
            "  Attempting uninstall: fastjsonschema\n",
            "    Found existing installation: fastjsonschema 2.19.0\n",
            "    Uninstalling fastjsonschema-2.19.0:\n",
            "      Successfully uninstalled fastjsonschema-2.19.0\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.17.0\n",
            "    Uninstalling zipp-3.17.0:\n",
            "      Successfully uninstalled zipp-3.17.0\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 4.0.9\n",
            "    Uninstalling widgetsnbextension-4.0.9:\n",
            "      Successfully uninstalled widgetsnbextension-4.0.9\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 1.6.4\n",
            "    Uninstalling websocket-client-1.6.4:\n",
            "      Successfully uninstalled websocket-client-1.6.4\n",
            "  Attempting uninstall: webcolors\n",
            "    Found existing installation: webcolors 1.13\n",
            "    Uninstalling webcolors-1.13:\n",
            "      Successfully uninstalled webcolors-1.13\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.1.0\n",
            "    Uninstalling urllib3-2.1.0:\n",
            "      Successfully uninstalled urllib3-2.1.0\n",
            "  Attempting uninstall: uri-template\n",
            "    Found existing installation: uri-template 1.3.0\n",
            "    Uninstalling uri-template-1.3.0:\n",
            "      Successfully uninstalled uri-template-1.3.0\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2023.3\n",
            "    Uninstalling tzdata-2023.3:\n",
            "      Successfully uninstalled tzdata-2023.3\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.13.0\n",
            "    Uninstalling traitlets-5.13.0:\n",
            "      Successfully uninstalled traitlets-5.13.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.3.3\n",
            "    Uninstalling tornado-6.3.3:\n",
            "      Successfully uninstalled tornado-6.3.3\n",
            "  Attempting uninstall: toolz\n",
            "    Found existing installation: toolz 0.12.0\n",
            "    Uninstalling toolz-0.12.0:\n",
            "      Successfully uninstalled toolz-0.12.0\n",
            "  Attempting uninstall: tinycss2\n",
            "    Found existing installation: tinycss2 1.2.1\n",
            "    Uninstalling tinycss2-1.2.1:\n",
            "      Successfully uninstalled tinycss2-1.2.1\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.5\n",
            "    Uninstalling soupsieve-2.5:\n",
            "      Successfully uninstalled soupsieve-2.5\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.0\n",
            "    Uninstalling sniffio-1.3.0:\n",
            "      Successfully uninstalled sniffio-1.3.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 68.2.2\n",
            "    Uninstalling setuptools-68.2.2:\n",
            "      Successfully uninstalled setuptools-68.2.2\n",
            "  Attempting uninstall: send2trash\n",
            "    Found existing installation: Send2Trash 1.8.2\n",
            "    Uninstalling Send2Trash-1.8.2:\n",
            "      Successfully uninstalled Send2Trash-1.8.2\n",
            "  Attempting uninstall: rpds-py\n",
            "    Found existing installation: rpds-py 0.13.0\n",
            "    Uninstalling rpds-py-0.13.0:\n",
            "      Successfully uninstalled rpds-py-0.13.0\n",
            "  Attempting uninstall: rfc3986-validator\n",
            "    Found existing installation: rfc3986-validator 0.1.1\n",
            "    Uninstalling rfc3986-validator-0.1.1:\n",
            "      Successfully uninstalled rfc3986-validator-0.1.1\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 25.1.1\n",
            "    Uninstalling pyzmq-25.1.1:\n",
            "      Successfully uninstalled pyzmq-25.1.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: pywinpty\n",
            "    Found existing installation: pywinpty 2.0.12\n",
            "    Uninstalling pywinpty-2.0.12:\n",
            "      Successfully uninstalled pywinpty-2.0.12\n",
            "  Attempting uninstall: python-json-logger\n",
            "    Found existing installation: python-json-logger 2.0.7\n",
            "    Uninstalling python-json-logger-2.0.7:\n",
            "      Successfully uninstalled python-json-logger-2.0.7\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: pymysql\n",
            "    Found existing installation: PyMySQL 1.1.0\n",
            "    Uninstalling PyMySQL-1.1.0:\n",
            "      Successfully uninstalled PyMySQL-1.1.0\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.17.0\n",
            "    Uninstalling Pygments-2.17.0:\n",
            "      Successfully uninstalled Pygments-2.17.0\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.21\n",
            "    Uninstalling pycparser-2.21:\n",
            "      Successfully uninstalled pycparser-2.21\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.6\n",
            "    Uninstalling psutil-5.9.6:\n",
            "      Successfully uninstalled psutil-5.9.6\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 3.0.41\n",
            "    Uninstalling prompt-toolkit-3.0.41:\n",
            "      Successfully uninstalled prompt-toolkit-3.0.41\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus-client 0.18.0\n",
            "    Uninstalling prometheus-client-0.18.0:\n",
            "      Successfully uninstalled prometheus-client-0.18.0\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.0.0\n",
            "    Uninstalling platformdirs-4.0.0:\n",
            "      Successfully uninstalled platformdirs-4.0.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 10.1.0\n",
            "    Uninstalling Pillow-10.1.0:\n",
            "      Successfully uninstalled Pillow-10.1.0\n",
            "  Attempting uninstall: parso\n",
            "    Found existing installation: parso 0.8.3\n",
            "    Uninstalling parso-0.8.3:\n",
            "      Successfully uninstalled parso-0.8.3\n",
            "  Attempting uninstall: pandocfilters\n",
            "    Found existing installation: pandocfilters 1.5.0\n",
            "    Uninstalling pandocfilters-1.5.0:\n",
            "      Successfully uninstalled pandocfilters-1.5.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "  Attempting uninstall: overrides\n",
            "    Found existing installation: overrides 7.4.0\n",
            "    Uninstalling overrides-7.4.0:\n",
            "      Successfully uninstalled overrides-7.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.2\n",
            "    Uninstalling numpy-1.26.2:\n",
            "      Successfully uninstalled numpy-1.26.2\n",
            "  Attempting uninstall: nest-asyncio\n",
            "    Found existing installation: nest-asyncio 1.5.8\n",
            "    Uninstalling nest-asyncio-1.5.8:\n",
            "      Successfully uninstalled nest-asyncio-1.5.8\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 3.0.2\n",
            "    Uninstalling mistune-3.0.2:\n",
            "      Successfully uninstalled mistune-3.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.1.3\n",
            "    Uninstalling MarkupSafe-2.1.3:\n",
            "      Successfully uninstalled MarkupSafe-2.1.3\n",
            "  Attempting uninstall: locket\n",
            "    Found existing installation: locket 1.0.0\n",
            "    Uninstalling locket-1.0.0:\n",
            "      Successfully uninstalled locket-1.0.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.5\n",
            "    Uninstalling kiwisolver-1.4.5:\n",
            "      Successfully uninstalled kiwisolver-1.4.5\n",
            "  Attempting uninstall: jupyterlab-widgets\n",
            "    Found existing installation: jupyterlab-widgets 3.0.9\n",
            "    Uninstalling jupyterlab-widgets-3.0.9:\n",
            "      Successfully uninstalled jupyterlab-widgets-3.0.9\n",
            "  Attempting uninstall: jupyterlab-pygments\n",
            "    Found existing installation: jupyterlab-pygments 0.2.2\n",
            "    Uninstalling jupyterlab-pygments-0.2.2:\n",
            "      Successfully uninstalled jupyterlab-pygments-0.2.2\n",
            "  Attempting uninstall: jsonpointer\n",
            "    Found existing installation: jsonpointer 2.4\n",
            "    Uninstalling jsonpointer-2.4:\n",
            "      Successfully uninstalled jsonpointer-2.4\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.10.0\n",
            "    Uninstalling fsspec-2023.10.0:\n",
            "      Successfully uninstalled fsspec-2023.10.0\n",
            "  Attempting uninstall: fqdn\n",
            "    Found existing installation: fqdn 1.5.1\n",
            "    Uninstalling fqdn-1.5.1:\n",
            "      Successfully uninstalled fqdn-1.5.1\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.44.3\n",
            "    Uninstalling fonttools-4.44.3:\n",
            "      Successfully uninstalled fonttools-4.44.3\n",
            "  Attempting uninstall: executing\n",
            "    Found existing installation: executing 2.0.1\n",
            "    Uninstalling executing-2.0.1:\n",
            "      Successfully uninstalled executing-2.0.1\n",
            "  Attempting uninstall: defusedxml\n",
            "    Found existing installation: defusedxml 0.7.1\n",
            "    Uninstalling defusedxml-0.7.1:\n",
            "      Successfully uninstalled defusedxml-0.7.1\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.1.1\n",
            "    Uninstalling decorator-5.1.1:\n",
            "      Successfully uninstalled decorator-5.1.1\n",
            "  Attempting uninstall: debugpy\n",
            "    Found existing installation: debugpy 1.8.0\n",
            "    Uninstalling debugpy-1.8.0:\n",
            "      Successfully uninstalled debugpy-1.8.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: colorama\n",
            "    Found existing installation: colorama 0.4.6\n",
            "    Uninstalling colorama-0.4.6:\n",
            "      Successfully uninstalled colorama-0.4.6\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.0.0\n",
            "    Uninstalling cloudpickle-3.0.0:\n",
            "      Successfully uninstalled cloudpickle-3.0.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.2\n",
            "    Uninstalling charset-normalizer-3.3.2:\n",
            "      Successfully uninstalled charset-normalizer-3.3.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.11.17\n",
            "    Uninstalling certifi-2023.11.17:\n",
            "      Successfully uninstalled certifi-2023.11.17\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "  Attempting uninstall: async-lru\n",
            "    Found existing installation: async-lru 2.0.4\n",
            "    Uninstalling async-lru-2.0.4:\n",
            "      Successfully uninstalled async-lru-2.0.4\n",
            "  Attempting uninstall: terminado\n",
            "    Found existing installation: terminado 0.18.0\n",
            "    Uninstalling terminado-0.18.0:\n",
            "      Successfully uninstalled terminado-0.18.0\n",
            "  Attempting uninstall: rfc3339-validator\n",
            "    Found existing installation: rfc3339-validator 0.1.4\n",
            "    Uninstalling rfc3339-validator-0.1.4:\n",
            "      Successfully uninstalled rfc3339-validator-0.1.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: referencing\n",
            "    Found existing installation: referencing 0.31.0\n",
            "    Uninstalling referencing-0.31.0:\n",
            "      Successfully uninstalled referencing-0.31.0\n",
            "  Attempting uninstall: qtpy\n",
            "    Found existing installation: QtPy 2.4.1\n",
            "    Uninstalling QtPy-2.4.1:\n",
            "      Successfully uninstalled QtPy-2.4.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: partd\n",
            "    Found existing installation: partd 1.4.1\n",
            "    Uninstalling partd-1.4.1:\n",
            "      Successfully uninstalled partd-1.4.1\n",
            "  Attempting uninstall: matplotlib-inline\n",
            "    Found existing installation: matplotlib-inline 0.1.6\n",
            "    Uninstalling matplotlib-inline-0.1.6:\n",
            "      Successfully uninstalled matplotlib-inline-0.1.6\n",
            "  Attempting uninstall: jupyter-core\n",
            "    Found existing installation: jupyter_core 5.5.0\n",
            "    Uninstalling jupyter_core-5.5.0:\n",
            "      Successfully uninstalled jupyter_core-5.5.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.2\n",
            "    Uninstalling Jinja2-3.1.2:\n",
            "      Successfully uninstalled Jinja2-3.1.2\n",
            "  Attempting uninstall: jedi\n",
            "    Found existing installation: jedi 0.19.1\n",
            "    Uninstalling jedi-0.19.1:\n",
            "      Successfully uninstalled jedi-0.19.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.8.0\n",
            "    Uninstalling importlib-metadata-6.8.0:\n",
            "      Successfully uninstalled importlib-metadata-6.8.0\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.2.0\n",
            "    Uninstalling contourpy-1.2.0:\n",
            "      Successfully uninstalled contourpy-1.2.0\n",
            "  Attempting uninstall: comm\n",
            "    Found existing installation: comm 0.2.0\n",
            "    Uninstalling comm-0.2.0:\n",
            "      Successfully uninstalled comm-0.2.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.16.0\n",
            "    Uninstalling cffi-1.16.0:\n",
            "      Successfully uninstalled cffi-1.16.0\n",
            "  Attempting uninstall: bleach\n",
            "    Found existing installation: bleach 6.1.0\n",
            "    Uninstalling bleach-6.1.0:\n",
            "      Successfully uninstalled bleach-6.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.12.2\n",
            "    Uninstalling beautifulsoup4-4.12.2:\n",
            "      Successfully uninstalled beautifulsoup4-4.12.2\n",
            "  Attempting uninstall: babel\n",
            "    Found existing installation: Babel 2.13.1\n",
            "    Uninstalling Babel-2.13.1:\n",
            "      Successfully uninstalled Babel-2.13.1\n",
            "  Attempting uninstall: asttokens\n",
            "    Found existing installation: asttokens 2.4.1\n",
            "    Uninstalling asttokens-2.4.1:\n",
            "      Successfully uninstalled asttokens-2.4.1\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.0.0\n",
            "    Uninstalling anyio-4.0.0:\n",
            "      Successfully uninstalled anyio-4.0.0\n",
            "  Attempting uninstall: stack-data\n",
            "    Found existing installation: stack-data 0.6.3\n",
            "    Uninstalling stack-data-0.6.3:\n",
            "      Successfully uninstalled stack-data-0.6.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.3\n",
            "    Uninstalling pandas-2.1.3:\n",
            "      Successfully uninstalled pandas-2.1.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.2\n",
            "    Uninstalling matplotlib-3.8.2:\n",
            "      Successfully uninstalled matplotlib-3.8.2\n",
            "  Attempting uninstall: jupyter-server-terminals\n",
            "    Found existing installation: jupyter_server_terminals 0.4.4\n",
            "    Uninstalling jupyter_server_terminals-0.4.4:\n",
            "      Successfully uninstalled jupyter_server_terminals-0.4.4\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter_client 8.6.0\n",
            "    Uninstalling jupyter_client-8.6.0:\n",
            "      Successfully uninstalled jupyter_client-8.6.0\n",
            "  Attempting uninstall: jsonschema-specifications\n",
            "    Found existing installation: jsonschema-specifications 2023.11.1\n",
            "    Uninstalling jsonschema-specifications-2023.11.1:\n",
            "      Successfully uninstalled jsonschema-specifications-2023.11.1\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2023.11.0\n",
            "    Uninstalling dask-2023.11.0:\n",
            "      Successfully uninstalled dask-2023.11.0\n",
            "  Attempting uninstall: arrow\n",
            "    Found existing installation: arrow 1.3.0\n",
            "    Uninstalling arrow-1.3.0:\n",
            "      Successfully uninstalled arrow-1.3.0\n",
            "  Attempting uninstall: argon2-cffi-bindings\n",
            "    Found existing installation: argon2-cffi-bindings 21.2.0\n",
            "    Uninstalling argon2-cffi-bindings-21.2.0:\n",
            "      Successfully uninstalled argon2-cffi-bindings-21.2.0\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.20.0\n",
            "    Uninstalling jsonschema-4.20.0:\n",
            "      Successfully uninstalled jsonschema-4.20.0\n",
            "  Attempting uninstall: isoduration\n",
            "    Found existing installation: isoduration 20.11.0\n",
            "    Uninstalling isoduration-20.11.0:\n",
            "      Successfully uninstalled isoduration-20.11.0\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 8.17.2\n",
            "    Uninstalling ipython-8.17.2:\n",
            "      Successfully uninstalled ipython-8.17.2\n",
            "  Attempting uninstall: argon2-cffi\n",
            "    Found existing installation: argon2-cffi 23.1.0\n",
            "    Uninstalling argon2-cffi-23.1.0:\n",
            "      Successfully uninstalled argon2-cffi-23.1.0\n",
            "  Attempting uninstall: nbformat\n",
            "    Found existing installation: nbformat 5.9.2\n",
            "    Uninstalling nbformat-5.9.2:\n",
            "      Successfully uninstalled nbformat-5.9.2\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 8.1.1\n",
            "    Uninstalling ipywidgets-8.1.1:\n",
            "      Successfully uninstalled ipywidgets-8.1.1\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 6.26.0\n",
            "    Uninstalling ipykernel-6.26.0:\n",
            "      Successfully uninstalled ipykernel-6.26.0\n",
            "  Attempting uninstall: qtconsole\n",
            "    Found existing installation: qtconsole 5.5.1\n",
            "    Uninstalling qtconsole-5.5.1:\n",
            "      Successfully uninstalled qtconsole-5.5.1\n",
            "  Attempting uninstall: nbclient\n",
            "    Found existing installation: nbclient 0.9.0\n",
            "    Uninstalling nbclient-0.9.0:\n",
            "      Successfully uninstalled nbclient-0.9.0\n",
            "  Attempting uninstall: jupyter-events\n",
            "    Found existing installation: jupyter-events 0.9.0\n",
            "    Uninstalling jupyter-events-0.9.0:\n",
            "      Successfully uninstalled jupyter-events-0.9.0\n",
            "  Attempting uninstall: jupyter-console\n",
            "    Found existing installation: jupyter-console 6.6.3\n",
            "    Uninstalling jupyter-console-6.6.3:\n",
            "      Successfully uninstalled jupyter-console-6.6.3\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 7.11.0\n",
            "    Uninstalling nbconvert-7.11.0:\n",
            "      Successfully uninstalled nbconvert-7.11.0\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter_server 2.10.1\n",
            "    Uninstalling jupyter_server-2.10.1:\n",
            "      Successfully uninstalled jupyter_server-2.10.1\n",
            "  Attempting uninstall: notebook-shim\n",
            "    Found existing installation: notebook_shim 0.2.3\n",
            "    Uninstalling notebook_shim-0.2.3:\n",
            "      Successfully uninstalled notebook_shim-0.2.3\n",
            "  Attempting uninstall: jupyterlab-server\n",
            "    Found existing installation: jupyterlab_server 2.25.2\n",
            "    Uninstalling jupyterlab_server-2.25.2:\n",
            "      Successfully uninstalled jupyterlab_server-2.25.2\n",
            "  Attempting uninstall: jupyter-lsp\n",
            "    Found existing installation: jupyter-lsp 2.2.0\n",
            "    Uninstalling jupyter-lsp-2.2.0:\n",
            "      Successfully uninstalled jupyter-lsp-2.2.0\n",
            "  Attempting uninstall: jupyterlab\n",
            "    Found existing installation: jupyterlab 4.0.8\n",
            "    Uninstalling jupyterlab-4.0.8:\n",
            "      Successfully uninstalled jupyterlab-4.0.8\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 7.0.6\n",
            "    Uninstalling notebook-7.0.6:\n",
            "      Successfully uninstalled notebook-7.0.6\n",
            "  Attempting uninstall: jupyter\n",
            "    Found existing installation: jupyter 1.0.0\n",
            "    Uninstalling jupyter-1.0.0:\n",
            "      Successfully uninstalled jupyter-1.0.0\n",
            "  Attempting uninstall: fynesse\n",
            "    Found existing installation: fynesse 0.1.1\n",
            "    Uninstalling fynesse-0.1.1:\n",
            "      Successfully uninstalled fynesse-0.1.1\n",
            "Successfully installed anyio-4.0.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 asttokens-2.4.1 async-lru-2.0.4 attrs-23.1.0 babel-2.13.1 beautifulsoup4-4.12.2 bleach-6.1.0 certifi-2023.11.17 cffi-1.16.0 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-3.0.0 colorama-0.4.6 comm-0.2.0 contourpy-1.2.0 cycler-0.12.1 dask-2023.11.0 debugpy-1.8.0 decorator-5.1.1 defusedxml-0.7.1 executing-2.0.1 fastjsonschema-2.19.0 fonttools-4.44.3 fqdn-1.5.1 fsspec-2023.10.0 fynesse-0.1.1 idna-3.4 importlib-metadata-6.8.0 ipykernel-6.26.0 ipython-8.17.2 ipywidgets-8.1.1 isoduration-20.11.0 jedi-0.19.1 jinja2-3.1.2 json5-0.9.14 jsonpointer-2.4 jsonschema-4.20.0 jsonschema-specifications-2023.11.1 jupyter-1.0.0 jupyter-client-8.6.0 jupyter-console-6.6.3 jupyter-core-5.5.0 jupyter-events-0.9.0 jupyter-lsp-2.2.0 jupyter-server-2.10.1 jupyter-server-terminals-0.4.4 jupyterlab-4.0.9 jupyterlab-pygments-0.2.2 jupyterlab-server-2.25.2 jupyterlab-widgets-3.0.9 kiwisolver-1.4.5 locket-1.0.0 markupsafe-2.1.3 matplotlib-3.8.2 matplotlib-inline-0.1.6 mistune-3.0.2 nbclient-0.9.0 nbconvert-7.11.0 nbformat-5.9.2 nest-asyncio-1.5.8 notebook-7.0.6 notebook-shim-0.2.3 numpy-1.26.2 overrides-7.4.0 packaging-23.2 pandas-2.1.3 pandocfilters-1.5.0 parso-0.8.3 partd-1.4.1 pillow-10.1.0 platformdirs-4.0.0 prometheus-client-0.18.0 prompt-toolkit-3.0.41 psutil-5.9.6 pure-eval-0.2.2 pycparser-2.21 pygments-2.17.1 pymysql-1.1.0 pyparsing-3.1.1 python-dateutil-2.8.2 python-json-logger-2.0.7 pytz-2023.3.post1 pywin32-306 pywinpty-2.0.12 pyyaml-6.0.1 pyzmq-25.1.1 qtconsole-5.5.1 qtpy-2.4.1 referencing-0.31.0 requests-2.31.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.13.0 send2trash-1.8.2 setuptools-68.2.2 six-1.16.0 sniffio-1.3.0 soupsieve-2.5 stack-data-0.6.3 terminado-0.18.0 tinycss2-1.2.1 toolz-0.12.0 tornado-6.3.3 traitlets-5.13.0 types-python-dateutil-2.8.19.14 tzdata-2023.3 uri-template-1.3.0 urllib3-2.1.0 wcwidth-0.2.10 webcolors-1.13 webencodings-0.5.1 websocket-client-1.6.4 wget-3.2 widgetsnbextension-4.0.9 zipp-3.17.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/jeffrey-22/ads.git 'C:\\Users\\86189\\AppData\\Local\\Temp\\pip-req-build-tzie631s'\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~.rnado'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~.zmq.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~.q'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~-ml'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~.util'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~umpy.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~arkupsafe'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~harset_normalizer'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~andas.libs'.\n",
            "  You can safely remove it manually.\n",
            "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\86189\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~andas'.\n",
            "  You can safely remove it manually.\n"
          ]
        }
      ],
      "source": [
        "# Install your library here, for example the fynesse template\n",
        "# is set up to be pip installable\n",
        "%pip install git+https://github.com/jeffrey-22/ads.git\n",
        "import fynesse\n",
        "# 2 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "pp_database_details = fynesse.access.retreive_database_details()\n",
        "# fynesse.access.create_database(pp_database_details)\n",
        "pp_database_conn = fynesse.access.create_connection(pp_database_details)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e789b174",
      "metadata": {
        "id": "e789b174"
      },
      "source": [
        "In the box below, briefly describe what the schema is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7d9d674",
      "metadata": {
        "id": "d7d9d674"
      },
      "source": [
        "```GIVE YOUR WRITTEN ANSWER HERE```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "92da8c96",
      "metadata": {
        "id": "92da8c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded files: {'tmp_data\\\\pp-2021-part2.csv', 'tmp_data\\\\pp-2014-part2.csv', 'tmp_data\\\\pp-1997-part1.csv', 'tmp_data\\\\pp-2010-part2.csv', 'tmp_data\\\\pp-2019-part2.csv', 'tmp_data\\\\pp-2021-part1.csv', 'tmp_data\\\\pp-2003-part2.csv', 'tmp_data\\\\pp-2020-part1.csv', 'tmp_data\\\\pp-2012-part1.csv', 'tmp_data\\\\pp-2022.csv', 'tmp_data\\\\pp-2011-part2.csv', 'tmp_data\\\\pp-2006-part2.csv', 'tmp_data\\\\pp-2018-part1.csv', 'tmp_data\\\\pp-2007-part2.csv', 'tmp_data\\\\pp-2005-part1.csv', 'tmp_data\\\\pp-1998-part2.csv', 'tmp_data\\\\pp-2013-part1.csv', 'tmp_data\\\\pp-2013-part2.csv', 'tmp_data\\\\pp-2009-part2.csv', 'tmp_data\\\\pp-2007-part1.csv', 'tmp_data\\\\pp-2006-part1.csv', 'tmp_data\\\\pp-2000-part2.csv', 'tmp_data\\\\pp-1997-part2.csv', 'tmp_data\\\\pp-1995-part2.csv', 'tmp_data\\\\pp-2005-part2.csv', 'tmp_data\\\\pp-2014-part1.csv', 'tmp_data\\\\pp-1998-part1.csv', 'tmp_data\\\\pp-2016-part1.csv', 'tmp_data\\\\pp-2016-part2.csv', 'tmp_data\\\\pp-1996-part1.csv', 'tmp_data\\\\pp-2011-part1.csv', 'tmp_data\\\\pp-2017-part1.csv', 'tmp_data\\\\pp-2018-part2.csv', 'tmp_data\\\\pp-2019-part1.csv', 'tmp_data\\\\pp-2017-part2.csv', 'tmp_data\\\\pp-2010-part1.csv', 'tmp_data\\\\pp-2002-part2.csv', 'tmp_data\\\\pp-2020-part2.csv', 'tmp_data\\\\pp-1996-part2.csv', 'tmp_data\\\\pp-1999-part1.csv', 'tmp_data\\\\pp-2003-part1.csv', 'tmp_data\\\\pp-2008-part1.csv', 'tmp_data\\\\pp-2001-part1.csv', 'tmp_data\\\\pp-2015-part1.csv', 'tmp_data\\\\pp-2000-part1.csv', 'tmp_data\\\\pp-2015-part2.csv', 'tmp_data\\\\pp-2009-part1.csv', 'tmp_data\\\\pp-2004-part1.csv', 'tmp_data\\\\pp-2001-part2.csv', 'tmp_data\\\\pp-1999-part2.csv', 'tmp_data\\\\pp-2002-part1.csv', 'tmp_data\\\\pp-2004-part2.csv', 'tmp_data\\\\pp-1995-part1.csv', 'tmp_data\\\\pp-2008-part2.csv', 'tmp_data\\\\pp-2012-part2.csv'}\n"
          ]
        }
      ],
      "source": [
        "# Write the code you need for creating the table, downloading and uploading the data here. You can use as many code blocks as you need.\n",
        "import os\n",
        "os.makedirs(\"tmp_data\", exist_ok=True)\n",
        "downloaded_pathnames = fynesse.access.download_price_data()\n",
        "print(f\"Downloaded files: {downloaded_pathnames}\")\n",
        "# 20 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fynesse.access.create_pp_table(pp_database_conn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 15m\n",
        "fynesse.access.upload_files_to_table(pp_database_conn, downloaded_pathnames, 'pp_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8m 30s\n",
        "fynesse.access.setup_pp_table(pp_database_conn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fynesse.access.select_count(pp_database_conn, 'pp_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fynesse.access.create_postcode_table(pp_database_conn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tmp_data\n"
          ]
        }
      ],
      "source": [
        "# 10s\n",
        "import requests\n",
        "def download_file_requests(url, output_filename):\n",
        "    # Slower than wget but at acceptable rate\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(output_filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "    except requests.HTTPError as http_err:\n",
        "        print(f\"HTTP Error: {http_err}\")\n",
        "    except Exception as err:\n",
        "        print(f\"Error during download: {err}\")\n",
        "def download_postcode_data(url = 'https://www.getthedata.com/downloads/open_postcode_geo.csv.zip'):\n",
        "    import os\n",
        "    postcode_zipname = os.path.join(\"tmp_data\", \"open_postcode_geo.csv.zip\")\n",
        "    postcode_filename = os.path.join(\"tmp_data\", \"open_postcode_geo.csv\")\n",
        "    pass\n",
        "    download_file_requests(url, postcode_zipname)\n",
        "\n",
        "    def unzip_file(zip_path):\n",
        "        import zipfile\n",
        "        extract_path = os.path.dirname(postcode_filename)\n",
        "        print(extract_path)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            pass\n",
        "            zip_ref.extractall(extract_path)\n",
        "\n",
        "    unzip_file(postcode_zipname)\n",
        "    return postcode_filename\n",
        "\n",
        "postcode_filename = download_postcode_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1m\n",
        "fynesse.access.upload_files_to_table(pp_database_conn, [postcode_filename], 'postcode_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1m\n",
        "fynesse.access.setup_postcode_table(pp_database_conn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67ca4ef",
      "metadata": {
        "id": "d67ca4ef"
      },
      "source": [
        "### Task D\n",
        "\n",
        "This table should contain the house price paid and the latitude and longitude of the house. We could create a new data frame that contains all this information. However, the computation of that data frame would take some time because of the size of the two existing tables in the join. Whether this is a good idea or not in a live system will depend on how often these predictions are required. If it's very often, it would be better to store the join as a new table in the database, because the one-off cost for that join is amortised across all the future predictions. If only a few predictions are required (like in our lab class) then doing that join on the fly might be better.\n",
        "\n",
        "- Option A: Think about which columns from each table will be useful to you in making predictions, then write code to efficiently select this information and create a data frame from the two tables for a set of properties. \"Join on the fly\". This option looks easier but the disadvantage is the extra latency it adds as joins are performed every time we need to answer data questions. These latencies are usually not accepted in production environments.\n",
        "\n",
        "- Option B: Alternatively, you can create a database table called `property_prices` to store the join of the tables you created in the previous tasks. The advantage of this approach is that you will get faster responses because the join is performed once. The disadvantage is that populating the new table can take a long time because you would join two big tables. You need to be more creative with this option. Remember that you can divide your dataset by different criteria (e.g., by year) and that loading data from files is much faster than `INSERT INTO` instructions. Populating the table took from 4 to 6 minutes per year in our tests on a Dell Laptop Intel Core i5 with 16GB of RAM and using the Eduroam network at the Computer Lab. Populating the table by year also gives you control over the upload process. You could write your code in a way you can stop and restart the upload process. The new table could have a schema like the one below:\n",
        "\n",
        "  ```\n",
        "  USE `property_prices`;\n",
        "  --\n",
        "  -- Table structure for table `prices_coordinates_data`\n",
        "  --\n",
        "  DROP TABLE IF EXISTS `prices_coordinates_data`;\n",
        "  CREATE TABLE IF NOT EXISTS `prices_coordinates_data` (\n",
        "    `price` int(10) unsigned NOT NULL,\n",
        "    `date_of_transfer` date NOT NULL,\n",
        "    `postcode` varchar(8) COLLATE utf8_bin NOT NULL,\n",
        "    `property_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "    `new_build_flag` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "    `tenure_type` varchar(1) COLLATE utf8_bin NOT NULL,\n",
        "    `locality` tinytext COLLATE utf8_bin NOT NULL,\n",
        "    `town_city` tinytext COLLATE utf8_bin NOT NULL,\n",
        "    `district` tinytext COLLATE utf8_bin NOT NULL,\n",
        "    `county` tinytext COLLATE utf8_bin NOT NULL,\n",
        "    `country` enum('England', 'Wales', 'Scotland', 'Northern Ireland', 'Channel Islands', 'Isle of Man') NOT NULL,\n",
        "    `latitude` decimal(11,8) NOT NULL,\n",
        "    `longitude` decimal(10,8) NOT NULL,\n",
        "    `db_id` bigint(20) unsigned NOT NULL\n",
        "  ) DEFAULT CHARSET=utf8 COLLATE=utf8_bin AUTO_INCREMENT=1 ;\n",
        "\n",
        "  ```\n",
        "\n",
        "In both cases you will need to perform a join between `pp_data` and `postcode_data` tables. Joining large tables without the right indexes in place will take a long time. You should think and set the right index for an efficient join. Indexing the `pp_data` table should take less than 5 minutes, while it takes less than one minute to index the `postcode_data` table.\n",
        "\n",
        "Note that there is no preference for either approach in the mark scheme.\n",
        "\n",
        "You should use the joined data in your prediction model in Question 3. Exploit the nature of the task to use prices for a particular region in a given period. This means we can select the relevant rows from the database according to that region and period.\n",
        "\n",
        "***After you have populated your database tables and created the functions to access the data you need for Question 3, you will not need to redo any of the previous steps. If at some point you find the AWS database is not responding or taking longer than expected to perform operations, you can have a look at the process list and kill the one are causing problems. If killing the processes does not work, you should reboot the database in the AWS console. Be careful with other database instances if you need to reboot your database. Also, be careful not to delete the database instead of rebooting it. If you delete the database, it is likely you will need to redo all Question 1.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y-lttyzUtSv7",
      "metadata": {
        "id": "Y-lttyzUtSv7"
      },
      "outputs": [],
      "source": [
        "# Write the code you used to join the tables, or the code you used to join on the fly.\n",
        "fynesse.access.create_prices_coordinates_table(pp_database_conn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_two_tables(price_table_pathname, postcode_table_pathname, joined_table_pathnames = set(), overwrite = True):\n",
        "    output_pathname = price_table_pathname.split('.')[0] + \"-joined.csv\"\n",
        "    if (overwrite or not (os.path.exists(output_pathname))):\n",
        "        import dask.dataframe as dd\n",
        "        column_names = [\n",
        "            'transaction_unique_identifier',\n",
        "            'price',\n",
        "            'date_of_transfer',\n",
        "            'postcode',\n",
        "            'property_type',\n",
        "            'new_build_flag',\n",
        "            'tenure_type',\n",
        "            'primary_addressable_object_name',\n",
        "            'secondary_addressable_object_name',\n",
        "            'street',\n",
        "            'locality',\n",
        "            'town_city',\n",
        "            'district',\n",
        "            'county',\n",
        "            'ppd_category_type',\n",
        "            'record_status'\n",
        "        ]\n",
        "        dtype_dict = {col: str for col in column_names}\n",
        "        table1 = dd.read_csv(price_table_pathname, header=None, names=column_names, dtype=dtype_dict)\n",
        "        table1 = table1[[\n",
        "            'postcode',\n",
        "            'price',\n",
        "            'date_of_transfer',\n",
        "            'property_type',\n",
        "            'new_build_flag',\n",
        "            'tenure_type',\n",
        "            'locality',\n",
        "            'town_city',\n",
        "            'district',\n",
        "            'county'\n",
        "        ]]\n",
        "        column_names = [\n",
        "            'postcode',\n",
        "            'status',\n",
        "            'usertype',\n",
        "            'easting',\n",
        "            'northing',\n",
        "            'positional_quality_indicator',\n",
        "            'country',\n",
        "            'latitude',\n",
        "            'longitude',\n",
        "            'postcode_no_space',\n",
        "            'postcode_fixed_width_seven',\n",
        "            'postcode_fixed_width_eight',\n",
        "            'postcode_area',\n",
        "            'postcode_district',\n",
        "            'postcode_sector',\n",
        "            'outcode',\n",
        "            'incode'\n",
        "        ]\n",
        "        dtype_dict = {col: str for col in column_names}\n",
        "        table2 = dd.read_csv(postcode_table_pathname, header=None, names=column_names, dtype=dtype_dict)\n",
        "        table2 = table2[[\n",
        "            'postcode',\n",
        "            'country',\n",
        "            'latitude',\n",
        "            'longitude'\n",
        "        ]]\n",
        "        result = dd.merge(table1, table2, on='postcode', how='inner')\n",
        "        result = result[[\n",
        "            'price',\n",
        "            'date_of_transfer',\n",
        "            'postcode',\n",
        "            'property_type',\n",
        "            'new_build_flag',\n",
        "            'tenure_type',\n",
        "            'locality',\n",
        "            'town_city',\n",
        "            'district',\n",
        "            'county',\n",
        "            'country',\n",
        "            'latitude',\n",
        "            'longitude'\n",
        "        ]]\n",
        "        result.to_csv(output_pathname, index=False, single_file=True)\n",
        "    joined_table_pathnames.add(output_pathname)\n",
        "    return joined_table_pathnames\n",
        "def join_all_tables(price_table_pathnames, postcode_table_pathname, joined_table_pathnames = set(), overwrite = True):\n",
        "    step = 0\n",
        "    for price_table_pathname in price_table_pathnames:\n",
        "        joined_table_pathnames = join_two_tables(price_table_pathname, postcode_table_pathname, joined_table_pathnames, overwrite)\n",
        "        step += 1\n",
        "    return joined_table_pathnames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "joined_table_pathnames = join_all_tables(downloaded_pathnames, postcode_filename, overwrite=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100.0% of files uploadedles uploadedd\r"
          ]
        }
      ],
      "source": [
        "# 10m\n",
        "fynesse.access.upload_files_to_table(pp_database_conn, joined_table_pathnames, 'prices_coordinates_data', ignore_first_row=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_prices_coordinates_table(conn):\n",
        "    with conn.cursor() as cursor:\n",
        "        query = '''\n",
        "        ALTER TABLE `prices_coordinates_data`\n",
        "        MODIFY `db_id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,AUTO_INCREMENT=1,\n",
        "        ADD PRIMARY KEY (`db_id`);\n",
        "        '''\n",
        "        cursor.execute(query)\n",
        "    conn.commit()\n",
        "# 9m\n",
        "setup_prices_coordinates_table(pp_database_conn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df36e5d1",
      "metadata": {
        "id": "df36e5d1"
      },
      "source": [
        "## Question 2. Accessing OpenStreetMap and Assessing the Available Features\n",
        "\n",
        "In question 3 you will be given the task of constructing a prediction system for property price levels at a given location. We expect that knowledge of the local region around the property should be helpful in making those price predictions. To evaluate this we will now look at [OpenStreetMap](https://www.openstreetmap.org) as a data source.\n",
        "\n",
        "The tasks below will guide you in accessing and assessing the OpenStreetMap data. The code you write will eventually be assimilated in your python module, but documentation of what you've included and why should remain in the notebook below.\n",
        "\n",
        "Accessing OpenStreetMap through its API can be done using the python library `osmx`. Using what you have learned about the `osmx` interface in the lectures, write general code for downloading points of interest and other relevant information that you believe may be useful for predicting house prices. Remembering the perspectives we've taken on *data science as debugging*, the remarks we've made when discussing *the data crisis* of the importance of reusability in data analysis, and the techniques we've explored in the labsessions for visualising features and exploring their correlation use the notebook to document your assessment of the OpenStreetMap data as a potential source of data.\n",
        "\n",
        "The knowledge you need to do a first pass through this question will have been taught by end of lab session three (16th November 2021). You will likely want to review your answer as part of *refactoring* your code and analysis pipeline shortly before hand in.\n",
        "\n",
        "You should write reusable code that allows you to explore the characteristics of different points of interest. Looking ahead to question 3 you'll want to incorporate these points of interest in your prediction code.\n",
        "\n",
        "*5 marks*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "042a2863",
      "metadata": {
        "id": "042a2863"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\86189\\AppData\\Local\\Temp\\ipykernel_12308\\1753380524.py:10: UserWarning: The `geometries` module and `geometries_from_X` functions have been renamed the `features` module and `features_from_X` functions. Use these instead. The `geometries` module and function names are deprecated and will be removed in a future release.\n",
            "  pois = ox.geometries_from_bbox(north=bounding_box['north'],\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Index(['barrier', 'bicycle', 'foot', 'motor_vehicle', 'geometry', 'addr:city',\n",
              "       'addr:postcode', 'addr:street', 'alt_name', 'amenity',\n",
              "       ...\n",
              "       'cctv', 'toilets:handwashing', 'fish_pass', 'navigable', 'stream',\n",
              "       'tidal', 'construction', 'ways', 'surface:listed_status',\n",
              "       'surface:source'],\n",
              "      dtype='object', length=492)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use this cell and cells below for summarising your analysis and documenting your decision making.\n",
        "def get_pois_from_bbox(bounding_box):\n",
        "    tag_list = {\"amenity\": True,\n",
        "                \"public_transport\": True,\n",
        "                \"landuse\": True,\n",
        "                \"natural\": True,\n",
        "                \"shop\": True,\n",
        "                \"leisure\": True}\n",
        "    try:\n",
        "        pois = ox.geometries_from_bbox(north=bounding_box['north'], \n",
        "                                        south=bounding_box['south'], \n",
        "                                        west=bounding_box['west'], \n",
        "                                        east=bounding_box['east'],\n",
        "                                        tags={tag: True for tag in tag_list})\n",
        "        return (len(pois), pois)\n",
        "    except:\n",
        "        return (0, ())\n",
        "def generate_bbox(latitude, longitude, box_height = 0.04, box_width = 0.04):\n",
        "    return {\n",
        "        'north': latitude + box_height / 2,\n",
        "        'south': latitude - box_height / 2,\n",
        "        'west': longitude - box_width / 2,\n",
        "        'east': longitude + box_width / 2\n",
        "    }\n",
        "def poi_feature_extraction(pois):\n",
        "    pass\n",
        "    \n",
        "\n",
        "pois = get_pois_from_bbox(generate_bbox(52.2062, 0.1186))\n",
        "pois.columns\n",
        "\n",
        "prepare training data\n",
        "\n",
        "PCA of training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a0e365",
      "metadata": {
        "id": "09a0e365"
      },
      "source": [
        "## Question 3. Addressing a Property Price Prediction Question\n",
        "\n",
        "For your final tick, we will be asking you to make house price predictions for a given location, date and property type in the UK. You will provide a function that takes input a latitude and longitude as well as the `property_type` (either type\" of property (either `F` - flat, `S` - semidetached, `D` - detached, `T` - terraced or `O` other). Create this function in the `address.py` file, for example in the form,\n",
        "\n",
        "```\n",
        "def predict_price(latitude, longitude, date, property_type):\n",
        "    \"\"\"Price prediction for UK housing.\"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "We suggest that you use the following approach when building your prediction.\n",
        "\n",
        "1. Select a bounding box around the housing location in latitude and longitude.\n",
        "2. Select a data range around the prediction date.\n",
        "3. Use the data ecosystem you have build above to build a training set from the relevant time period and location in the UK. Include appropriate features from OSM to improve the prediction.\n",
        "4. Train a linear model on the data set you have created.\n",
        "5. Validate the quality of the model.\n",
        "6. Provide a prediction of the price from the model, warning appropriately if your validation indicates the quality of the model is poor.\n",
        "\n",
        "Please note that the quality of predictions is not the main focus of the assignment - we expect to see models that output reasonable predictions and have positive R^2's, but you should not spend too much time on increasing the model's accuracy.\n",
        "\n",
        "The knowledge you need to do a first pass through this question will have been taught by end of lab session four (7th November 2023). You will likely want to review your answer as part of *refactoring* your code shortly before hand in.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Si3K0A2zM7pa",
      "metadata": {
        "id": "Si3K0A2zM7pa"
      },
      "outputs": [],
      "source": [
        "def predict_price(latitude, longitude, date, property_type, embeddings):\n",
        "    features = get_feature_array()\n",
        "    (warning, target) = calculate(embeddings, features)\n",
        "    return (warning, target)\n",
        "\n",
        "cross_validation_calc_Rsqr()\n",
        "\n",
        "ideal_embeddings = calc_ideal_emb()\n",
        "\n",
        "# example prediction plot of cambridge loc vs heat map of price"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eEDpLiW0Zr_L",
      "metadata": {
        "id": "eEDpLiW0Zr_L"
      },
      "source": [
        "## Large Language Models\n",
        "\n",
        "If you used LLMs to generate or fix code in this assignment (recommended), briefly summarise the process and prompts you used. What do you think of the integration of LLMs in the data science pipeline?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "osdBm1BNaHL1",
      "metadata": {
        "id": "osdBm1BNaHL1"
      },
      "source": [
        "```GIVE YOUR WRITTEN ANSWER HERE```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DJDQPmkLldvp",
      "metadata": {
        "id": "DJDQPmkLldvp"
      },
      "source": [
        "### FAQs\n",
        "\n",
        "- Why is my connection to the AWS server intermittent?\n",
        "\n",
        "If you are using Google Colab, online notebooks may throttle your code or time you out. Local Python code is recommended for tasks for large data management in the database.\n",
        "\n",
        "- Why do SQL queries take a long time?\n",
        "\n",
        "Some queries legitimately take a long time, even when done right. We suggest indexing your tables to speed up queries over your database. You can index tables by different columns depending on the queries you want to perform. For example, indexing the tables by `postcode` could speed up the join in Task D. More information regarding indexing in MariaDB is available [here](https://mariadb.com/kb/en/getting-started-with-indexes/).\n",
        "\n",
        "You may also want to consider creating single or multi-column indices on coordinates, or any other properties you use to select data, if that step is taking a long time.\n",
        "\n",
        "If your new queries seem stuck, try running `SHOW FULL PROCESSLIST`, and `KILL` any stuck processes.\n",
        "\n",
        "- Why are table populating processes taking so long?\n",
        "\n",
        "Again populating the database can take long. However, be careful if you are indexing the tables. You should populate data before indexing. Insert operations are impacted by indexes as they are updated with each new row inserted into the table.\n",
        "\n",
        "- Some other questions are answered in [this reddit forum](https://www.reddit.com/r/CST_ADS/) or [this doc](https://docs.google.com/document/d/1GfDROyUW8HVs2eyxmJzKrYGRdVyUiVXzPcDfwOO8wX0/edit?usp=sharing). Feel free to also ask about anything that comes up."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
